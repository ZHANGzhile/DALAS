{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4zarSeAQdHm"
      },
      "outputs": [],
      "source": [
        "!pip install -q lxml\n",
        "\n",
        "import bs4\n",
        "import lxml\n",
        "import pandas\n",
        "import urllib\n",
        "\n",
        "from urllib import request"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 1 : Scrap a table on a wikipedia page\n",
        "\n",
        "We would like to display on a map the location of Summer Olympic Games since 1896. We will use a [Wikipedia page](https://fr.wikipedia.org/wiki/Jeux_olympiques) to scrap the associated table.\n",
        "Below is the code to extract the content of the page using `request` and display its title using `BeautifulSoup`."
      ],
      "metadata": {
        "id": "t4wzROqzT36G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jo = \"https://fr.wikipedia.org/wiki/Jeux_olympiques\"\n",
        "\n",
        "request_text = request.urlopen(jo).read()\n",
        "print(request_text[:1000])\n",
        "page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
        "print(page.find(\"title\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl9QICzwTGG7",
        "outputId": "342c30b0-2ade-4dba-efb2-930fb307bb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'<!DOCTYPE html>\\n<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-enabled vector-feature-custom-font-size-clientpref-0 vector-feature-client-preferences-disabled vector-feature-client-prefs-pinned-disabled vector-toc-available\" lang=\"fr\" dir=\"ltr\">\\n<head>\\n<meta charset=\"UTF-8\">\\n<title>Jeux olympiques \\xe2\\x80\\x94 Wikip\\xc3\\xa9dia</title>\\n<script>(function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-c'\n",
            "<title>Jeux olympiques — Wikipédia</title>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scrapping in Python\n",
        "\n",
        "\n",
        "**TODO pour moi avant le TME**\n",
        "\n",
        "1.   Faire la version étudiant en enlevant les réponses. Ne garder qu'un code générique pour la carte qu'ils devront adapter\n",
        "\n",
        "\n",
        "\n",
        "The first objective of this notebook is to discover the `request` and `BeautifulSoup` libraries to crawl a table on a Wikitable page, build a dataframe, and create a map.\n",
        "\n",
        "*   `request` and [urllib](https://docs.python.org/3/library/urllib.html#module-urllib) for requestion REST API\n",
        "*   [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to inspect webpages\n",
        "\n",
        "Please note that this part serves as an initiation of web scrapping but you will need to learn by yourself to make the project. It is inspired from notebooks published by Galiana, Lino. 2023. Python Pour La Data Science. https://doi.org/10.5281/zenodo.8229676. Exercices target different sources of information, the code needs to be adapted.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EOr-DwYgRsUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our objective here is to extract the different information in the first table \"Jeux olympiques d'été\" and to build a data frame.\n",
        "\n",
        "To proceed, you will have to follow these steps:\n",
        "*   Find the table\n",
        "*   Collect each line of the table\n",
        "*   Retrieve the different columns and transform them into text format. Also, use `strip` to format the value into a proper text format (e.g., without useless spaces). Store these lines (formated as a table of columns) in a table.\n",
        "\n",
        "**Warning: We have two location for 1940. You have therefore to manage this case: identify the attribute indicating this fact and write a code to collect those lines and insert in the previous table in the right place**\n",
        "\n",
        "* Collect headers of the HTML table\n",
        "* Build a data frame from the result table and the headers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U_18h3E9UiYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#[student]"
      ],
      "metadata": {
        "id": "M5oVOOZuKxD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 2: Locate places on a map\n",
        "\n",
        "The objective here is to identify organizer cities on a map.\n",
        "You will have to code the following steps:\n",
        "* Collect the URL of each city\n",
        "* Go the this page using `urllib.request`\n",
        "* Find the coordinates\n",
        "* Store cities and coordinates in a data frame"
      ],
      "metadata": {
        "id": "Qw6i-yLueZvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#[student]"
      ],
      "metadata": {
        "id": "7xaYx79JMF_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then transform degree coordinates into numerical coordinates to build a map\n"
      ],
      "metadata": {
        "id": "4gqYroUff1UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def dms2dd(degrees, minutes, seconds, direction):\n",
        "    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n",
        "    if direction in ('S', 'O'):\n",
        "        dd *= -1\n",
        "    return dd\n",
        "\n",
        "def parse_dms(dms):\n",
        "    parts = re.split('[^\\d\\w]+', dms)\n",
        "    #potentially needs an adaptation\n",
        "    #[student]\n",
        "    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n",
        "    #lng = dms2dd(parts[4], parts[5], parts[6], parts[7])\n",
        "    return lat\n",
        "\n",
        "data['latitude'] = data['latitude'].apply(parse_dms)\n",
        "#data['longitude'] = data['longitude'].apply(parse_dms)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "SPkGBLnVWZWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The map can be obtain with the following code"
      ],
      "metadata": {
        "id": "j3_3JmXggIEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "from pathlib import Path\n",
        "import folium\n",
        "from IPython.display import display\n",
        "\n",
        "gdf = gpd.GeoDataFrame(\n",
        "    data, geometry=gpd.points_from_xy(data.longitude, data.latitude))\n",
        "\n",
        "Path(\"leaflet\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "center = gdf[['latitude', 'longitude']].mean().values.tolist()\n",
        "sw = gdf[['latitude', 'longitude']].min().values.tolist()\n",
        "ne = gdf[['latitude', 'longitude']].max().values.tolist()\n",
        "\n",
        "m = folium.Map(location = center, tiles='openstreetmap')\n",
        "\n",
        "# I can add marker one by one on the map\n",
        "for i in range(0,len(gdf)):\n",
        "    folium.Marker([gdf.iloc[i]['latitude'], gdf.iloc[i]['longitude']], popup=gdf.iloc[i]['name']).add_to(m)\n",
        "\n",
        "m.fit_bounds([sw, ne])\n",
        "display(m)"
      ],
      "metadata": {
        "id": "93_lzZI3Wkpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project\n",
        "\n",
        "The objective of this project is to practice all concepts taught in the main lecture. Therefore, you will have to collect data around a thematic, identify a problematic, clean and format the data, provide some exploratory analysis and visualization, build models and evaluate them. You will have also to design dashboard and to storytell the whole pipeline.\n",
        "\n",
        "\n",
        "The whole project will have two outputs describing the methodology and the results:\n",
        "* A technical report targeting your datascientist colleagues\n",
        "* An oral presentation targeting your CEO, chief, client. In this case, we assume that the audience is not specialized in data science. (But you also need to present the methodology to convince them to trust the results).\n",
        "\n",
        "\n",
        "**Requests:**\n",
        "* Team work of two people (same group throughout the semester)\n",
        "* All your work should be stored on a git repo: [tutorial](https://github.com/baskiotisn/2IN013robot2023/blob/d979333fb80c9b6acd9515aaec040943d10d365c/docs/tutoriel_git.pdf)\n",
        "\n",
        "**Remarks:**\n",
        "There are also other libraries or issues you'll encounter when collecting data. Some of these are listed below, but don't be shy and interact with a search engine to solve your own issue!\n",
        "* [Regular expressions](https://docs.python.org/3/howto/regex.html) might be useful!\n",
        "* [API with authentication](https://www.geeksforgeeks.org/authentication-using-python-requests/)\n",
        "*   [Selenium](https://selenium-python.readthedocs.io/) when pages are generated wia javascript scripts\n",
        "* [Playwright](https://playwright.dev/) --> looks easier and more adapted than Selenium\n",
        "* [Scrapy](https://scrapy.org/) for web crawling / or when you don't know the URL.[Tuto here](https://doc.scrapy.org/en/latest/intro/tutorial.html)\n",
        "* [Summary of some difficulties in scrapping data from the web](https://www.zenrows.com/blog/web-scraping-challenges#page-structure-changes)\n",
        "\n",
        "## Your daily task\n",
        "Collecting data from the web and open data portals. Open data might include csv files, you can use them, but be aware that scraping should be your main acitivity in the dataset gathering.\n",
        "You can begin to format the data, merge them to build dataframe that would be analyzed next week."
      ],
      "metadata": {
        "id": "BX4011aO3h4f"
      }
    }
  ]
}